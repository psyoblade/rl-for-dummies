{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "from six import StringIO, b\n",
    "\n",
    "from gym import utils\n",
    "from gym.envs.toy_text import discrete\n",
    "\n",
    "LEFT = 0\n",
    "DOWN = 1\n",
    "RIGHT = 2\n",
    "UP = 3\n",
    "\n",
    "MAPS = {\n",
    "    \"4x4\": [\n",
    "        \"SFFF\",\n",
    "        \"FHFH\",\n",
    "        \"FFFH\",\n",
    "        \"HFFG\"\n",
    "    ],\n",
    "    \"8x8\": [\n",
    "        \"SFFFFFFF\",\n",
    "        \"FFFFFFFF\",\n",
    "        \"FFFHFFFF\",\n",
    "        \"FFFFFHFF\",\n",
    "        \"FFFHFFFF\",\n",
    "        \"FHHFFFHF\",\n",
    "        \"FHFFHFHF\",\n",
    "        \"FFFHFFFG\"\n",
    "    ],\n",
    "}\n",
    "\n",
    "class FrozenLakeEnv(discrete.DiscreteEnv):\n",
    "    \"\"\"\n",
    "    Winter is here. You and your friends were tossing around a frisbee at the park\n",
    "    when you made a wild throw that left the frisbee out in the middle of the lake.\n",
    "    The water is mostly frozen, but there are a few holes where the ice has melted.\n",
    "    If you step into one of those holes, you'll fall into the freezing water.\n",
    "    At this time, there's an international frisbee shortage, so it's absolutely imperative that\n",
    "    you navigate across the lake and retrieve the disc.\n",
    "    However, the ice is slippery, so you won't always move in the direction you intend.\n",
    "    The surface is described using a grid like the following\n",
    "        SFFF\n",
    "        FHFH\n",
    "        FFFH\n",
    "        HFFG\n",
    "    S : starting point, safe\n",
    "    F : frozen surface, safe\n",
    "    H : hole, fall to your doom\n",
    "    G : goal, where the frisbee is located\n",
    "    The episode ends when you reach the goal or fall in a hole.\n",
    "    You receive a reward of 1 if you reach the goal, and zero otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {'render.modes': ['human', 'ansi']}\n",
    "\n",
    "    def __init__(self, desc=None, map_name=\"4x4\",is_slippery=True):\n",
    "        if desc is None and map_name is None:\n",
    "            raise ValueError('Must provide either desc or map_name')\n",
    "        elif desc is None:\n",
    "            desc = MAPS[map_name]\n",
    "        self.desc = desc = np.asarray(desc,dtype='c')\n",
    "        self.nrow, self.ncol = nrow, ncol = desc.shape\n",
    "        self.reward_range = (0, 1)\n",
    "\n",
    "        nA = 4\n",
    "        nS = nrow * ncol\n",
    "\n",
    "        isd = np.array(desc == b'S').astype('float64').ravel()\n",
    "        isd /= isd.sum()\n",
    "\n",
    "        P = {s : {a : [] for a in range(nA)} for s in range(nS)}\n",
    "\n",
    "        def to_s(row, col):\n",
    "            return row*ncol + col\n",
    "        \n",
    "        def inc(row, col, a):\n",
    "            if a==0: # left\n",
    "                col = max(col-1,0)\n",
    "            elif a==1: # down\n",
    "                row = min(row+1,nrow-1)\n",
    "            elif a==2: # right\n",
    "                col = min(col+1,ncol-1)\n",
    "            elif a==3: # up\n",
    "                row = max(row-1,0)\n",
    "            return (row, col)\n",
    "\n",
    "        for row in range(nrow):\n",
    "            for col in range(ncol):\n",
    "                s = to_s(row, col)\n",
    "                for a in range(4):\n",
    "                    li = P[s][a]\n",
    "                    letter = desc[row, col]\n",
    "                    if letter in b'GH':\n",
    "                        li.append((1.0, s, 0, True))\n",
    "                    else:\n",
    "                        if is_slippery:\n",
    "                            for b in [(a-1)%4, a, (a+1)%4]:\n",
    "                                newrow, newcol = inc(row, col, b)\n",
    "                                newstate = to_s(newrow, newcol)\n",
    "                                newletter = desc[newrow, newcol]\n",
    "                                done = bytes(newletter) in b'GH'\n",
    "                                rew = float(newletter == b'G')\n",
    "                                li.append((1.0/3.0, newstate, rew, done))\n",
    "                        else:\n",
    "                            newrow, newcol = inc(row, col, a)\n",
    "                            newstate = to_s(newrow, newcol)\n",
    "                            newletter = desc[newrow, newcol]\n",
    "                            done = bytes(newletter) in b'GH'\n",
    "                            rew = float(newletter == b'G')\n",
    "                            li.append((1.0, newstate, rew, done))\n",
    "\n",
    "        super(FrozenLakeEnv, self).__init__(nS, nA, P, isd)\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        outfile = StringIO() if mode == 'ansi' else sys.stdout\n",
    "\n",
    "        row, col = self.s // self.ncol, self.s % self.ncol\n",
    "        desc = self.desc.tolist()\n",
    "        desc = [[c.decode('utf-8') for c in line] for line in desc]\n",
    "        desc[row][col] = utils.colorize(desc[row][col], \"red\", highlight=True)\n",
    "        if self.lastaction is not None:\n",
    "            outfile.write(\"  ({})\\n\".format([\"Left\",\"Down\",\"Right\",\"Up\"][self.lastaction]))\n",
    "        else:\n",
    "            outfile.write(\"\\n\")\n",
    "        outfile.write(\"\\n\".join(''.join(line) for line in desc)+\"\\n\")\n",
    "\n",
    "        if mode != 'human':\n",
    "            return outfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector = [3, 2, 3, 3]\n",
    "x = np.max(vector)\n",
    "indices = np.nonzero(vector == x)[0] # 인덱스 자체가 랜덤 방향이 되므로.\n",
    "import random\n",
    "random.choice(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already Regitered\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import random as rand\n",
    "import numpy as np\n",
    "\n",
    "FROZENLAKE_NOT_SLIPPERY = 'FrozenLakeNotSlippery-v0'\n",
    "SLIPPERY='FrozenLake-v0'\n",
    "\n",
    "def register_frozen_lake_not_slippery(name):\n",
    "    from gym.envs.registration import register\n",
    "    register(\n",
    "        id=name,\n",
    "        entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "        kwargs={'map_name' : '4x4', 'is_slippery': False},\n",
    "        max_episode_steps=100,\n",
    "        reward_threshold=0.78, # optimum = .8196\n",
    "    )\n",
    "    \n",
    "DEBUG=False\n",
    "def debug(env, log):\n",
    "    if DEBUG and env != None:\n",
    "        env.render()\n",
    "    if DEBUG:\n",
    "        print(log)\n",
    "        \n",
    "def info(env, log):\n",
    "    env.render()\n",
    "    print(log)\n",
    "    \n",
    "def greedy_action(vector):\n",
    "    m = np.amax(vector)\n",
    "    indices = np.nonzero(vector == m)[0]\n",
    "    return rand.choice(indices)\n",
    "\n",
    "def random_action():\n",
    "    dirs = [ LEFT, DOWN, RIGHT, UP ]\n",
    "    return rand.choice(dirs)\n",
    "\n",
    "def exists_env(name):\n",
    "    from gym import envs\n",
    "    for env in envs.registry.all():\n",
    "        if env.id == name:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "if not exists_env(FROZENLAKE_NOT_SLIPPERY):\n",
    "    register_frozen_lake_not_slippery(FROZENLAKE_NOT_SLIPPERY)\n",
    "    print(\"Registered\")\n",
    "else:\n",
    "    print(\"Already Regitered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "left, down, right, up\n",
      "[0] [ 0.  1.  0.  0.]\n",
      "[1] [ 0.  0.  0.  0.]\n",
      "[2] [ 0.  0.  0.  0.]\n",
      "[3] [ 0.  0.  0.  0.]\n",
      "[4] [ 0.  1.  0.  0.]\n",
      "[5] [ 0.  0.  0.  0.]\n",
      "[6] [ 0.  0.  0.  0.]\n",
      "[7] [ 0.  0.  0.  0.]\n",
      "[8] [ 0.  0.  1.  0.]\n",
      "[9] [ 0.  1.  0.  0.]\n",
      "[10] [ 0.  0.  0.  0.]\n",
      "[11] [ 0.  0.  0.  0.]\n",
      "[12] [ 0.  0.  0.  0.]\n",
      "[13] [ 0.  0.  1.  0.]\n",
      "[14] [ 0.  0.  1.  0.]\n",
      "[15] [ 0.  0.  0.  0.]\n",
      "total reward is 2806\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(FROZENLAKE_NOT_SLIPPERY)\n",
    "\n",
    "LEFT = 0\n",
    "DOWN = 1\n",
    "RIGHT = 2\n",
    "UP = 3\n",
    "\n",
    "lr = .8\n",
    "y = .95\n",
    "num_episodes = 3000\n",
    "answers=[]\n",
    "Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "total = 0.0\n",
    "for step in range(num_episodes):\n",
    "    prev_state = env.reset()\n",
    "    doomed = False\n",
    "    state_list = []\n",
    "    while not doomed:\n",
    "        state_list.append(prev_state)\n",
    "        action = greedy_action(Q[prev_state,:])\n",
    "        curr_state, reward, doomed, _ = env.step(action)\n",
    "        if reward == 1.0:\n",
    "            debug(env, \"[SUCC][%d][%d->%d] : %f - %a\" % (step, prev_state, curr_state, reward, doomed))\n",
    "        elif doomed:\n",
    "            debug(env, \"[FAIL][%d][%d->%d] : %f\\n\" % (step, prev_state, curr_state, reward))\n",
    "            break\n",
    "        Q[prev_state, action] = reward + np.max(Q[curr_state,:])\n",
    "        prev_state = curr_state\n",
    "        total += reward\n",
    "print(\"left, down, right, up\")\n",
    "for x in range(len(Q)):\n",
    "    print(\"[%d] %s\" % (x, Q[x,:]))\n",
    "print(\"total reward is %d\" % total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Q-Table Values\n",
      "[[  5.82698888e-03   8.10405079e-03   2.05724807e-01   6.80835127e-03]\n",
      " [  5.77328482e-04   3.51814810e-04   6.66929863e-05   4.72895880e-02]\n",
      " [  8.91247325e-04   2.91184848e-03   3.56435794e-03   7.79812719e-03]\n",
      " [  0.00000000e+00   1.34757663e-03   3.30683431e-04   5.61992823e-03]\n",
      " [  3.60935889e-01   2.81086590e-03   2.03756917e-03   6.33905578e-04]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  5.38297333e-02   3.06025282e-09   6.57390573e-06   2.11311507e-05]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  5.96605729e-04   1.09666568e-04   2.94370580e-04   6.24011457e-01]\n",
      " [  3.19626281e-04   3.39689390e-01   9.44021307e-04   1.28309177e-03]\n",
      " [  8.11139968e-01   5.36157168e-04   4.76236270e-05   1.16402982e-03]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   2.76219483e-03   9.34150118e-01   0.00000000e+00]\n",
      " [  0.00000000e+00   9.91936993e-01   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "#Initialize table with all zeros\n",
    "Q = np.zeros([env.observation_space.n,env.action_space.n])\n",
    "# Set learning parameters\n",
    "lr = .8\n",
    "y = .95\n",
    "num_episodes = 2000\n",
    "#create lists to contain total rewards and steps per episode\n",
    "#jList = []\n",
    "rList = []\n",
    "for i in range(num_episodes):\n",
    "    #Reset environment and get first new observation\n",
    "    s = env.reset()\n",
    "    rAll = 0\n",
    "    d = False\n",
    "    j = 0\n",
    "    #The Q-Table learning algorithm\n",
    "    while j < 99:\n",
    "        j+=1\n",
    "        #Choose an action by greedily (with noise) picking from Q table\n",
    "        a = np.argmax(Q[s,:] + np.random.randn(1,env.action_space.n)*(1./(i+1)))\n",
    "        #Get new state and reward from environment\n",
    "        s1,r,d,_ = env.step(a)\n",
    "        #Update Q-Table with new knowledge\n",
    "        Q[s,a] = Q[s,a] + lr*(r + y*np.max(Q[s1,:]) - Q[s,a])\n",
    "        rAll += r\n",
    "        s = s1\n",
    "        if d == True:\n",
    "            break\n",
    "    #jList.append(j)\n",
    "    rList.append(rAll)\n",
    "    \n",
    "\n",
    "print(\"Final Q-Table Values\")\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(1 == 1.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
