{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving CartPole problem using PG\n",
    "> PG 알고리즘을 이용하여 CartPole 문제를 풀어보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, gym, copy\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from gym import envs\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class REINFORCEAgent:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.action_size = env.action_space.n\n",
    "        self.observation_space = env.observation_space.shape[0]\n",
    "        self.discount_factor = 0.99\n",
    "        self.learning_rate = 0.001\n",
    "        self.states, self.actions, self.rewards = [], [], []\n",
    "        self.model = self.build_model()\n",
    "        self.optimizer = self.build_optimizer()\n",
    "        print(\"action:\", self.action_size, \", state: \", self.observation_space)\n",
    "        \n",
    "    def __del__(self):\n",
    "        pass\n",
    "    \n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.observation_space, activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='softmax'))\n",
    "        model.summary()\n",
    "        return model\n",
    "    \n",
    "    def build_optimizer(self):\n",
    "        action = K.placeholder(shape=[None, self.action_size])\n",
    "        discounted_rewards = K.placeholder(shape=[None,])\n",
    "        # cross-entropy\n",
    "        action_prob = K.sum(action * self.model.output, axis=1)\n",
    "        cross_entropy = K.log(action_prob) * discounted_rewards\n",
    "        loss = -K.sum(cross_entropy)\n",
    "        # training-function\n",
    "        optimizer = Adam(lr=self.learning_rate)\n",
    "        updates = optimizer.get_updates(self.model.trainable_weights, [], loss)\n",
    "        train = K.function([self.model.input, action, discounted_rewards], [], updates=updates)\n",
    "        return train\n",
    "    \n",
    "    def append_sample(self, state, action, reward):\n",
    "        self.states.append(state[0]) # bugfix, state[0] 대신 state 를 넣었음\n",
    "        act = np.zeros(self.action_size)\n",
    "        act[action] = 1\n",
    "        self.actions.append(act)\n",
    "        self.rewards.append(reward)\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        policy = self.model.predict(state)[0]\n",
    "        return np.random.choice(self.action_size, 1, p=policy)[0]\n",
    "    \n",
    "    def discount_rewards(self, rewards):\n",
    "        discounted_rewards = np.zeros_like(rewards)\n",
    "        running_add = 0\n",
    "        for t in reversed(range(0, len(rewards))):\n",
    "            running_add = running_add * self.discount_factor + rewards[t]\n",
    "            discounted_rewards[t] = running_add\n",
    "        return discounted_rewards\n",
    "    \n",
    "    def train_model(self):\n",
    "        print('rewards.shape', type(self.rewards))\n",
    "        print('discount_rewards', type(self.discount_rewards(self.rewards)))\n",
    "        discounted_rewards = np.float32(self.discount_rewards(self.rewards))\n",
    "        discounted_rewards -= np.mean(discounted_rewards)\n",
    "        discounted_rewards /= np.std(discounted_rewards)\n",
    "        print('discounted_rewards', type(discounted_rewards), discounted_rewards.shape)\n",
    "        print('states', type(self.states), len(self.states))\n",
    "        print('actions', type(self.actions))\n",
    "        print('rewards', type(self.rewards))\n",
    "        \n",
    "        self.optimizer([self.states, self.actions, discounted_rewards])\n",
    "        self.states, self.actions, self.rewards = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def learn(max_episodes, render, reset):\n",
    "    env = gym.make('CartPole-v1')\n",
    "    agent = REINFORCEAgent(env)\n",
    "    global_step = 0\n",
    "    episodes, scores = [], []\n",
    "    \n",
    "    for e in range(max_episodes+1):\n",
    "        if render:\n",
    "            env.render()\n",
    "            \n",
    "        done = False\n",
    "        score = 0\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1,4])\n",
    "        print(state)\n",
    "        break\n",
    "        \n",
    "        while not done:\n",
    "            global_step += 1\n",
    "            \n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            agent.append_sample(state, action, reward)\n",
    "            \n",
    "            score += reward\n",
    "            next_state = np.reshape(next_state, [1,4])\n",
    "            state = copy.deepcopy(next_state)\n",
    "            \n",
    "            if done:\n",
    "                agent.train_model()\n",
    "                scores.append(score)\n",
    "                episodes.append(e)\n",
    "                print(\"episodes:\", e, \" score:\", score, \" time_step:\" , global_step)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_58 (Dense)             (None, 24)                120       \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_60 (Dense)             (None, 2)                 50        \n",
      "=================================================================\n",
      "Total params: 770\n",
      "Trainable params: 770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "action: 2 , state:  4\n",
      "[[ 0.03144073 -0.0032607   0.00020903 -0.04581364]]\n"
     ]
    }
   ],
   "source": [
    "learn(1000, True, False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [py36]",
   "language": "python",
   "name": "Python [py36]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
