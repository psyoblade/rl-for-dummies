{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning of Behaviors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goals\n",
    "#### 1. Understand definition & notations\n",
    "#### 2. Understand basic imitation learning algorithms\n",
    "#### 3. Understand their strength & weaknesses\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Definition of sequential decision problem\n",
    "> 1. 강화학습은 마치 사람처럼 환경과 상호작용 하면서 스스로 학습하는 방식입니다.\n",
    "> 2. 또한 순차적으로 해결 혹은 의사 결정해야 하는 문제에 적용할 수 있습니다. \n",
    "> 3. 이러한 순차적 의사 결정 문제를 보다 체계적으로 이해하고 풀기 위해서는 수학적인 표현이 가능해야만 합니다.\n",
    "> 4. 이럴 때에 사용하는 방법이 MDP (Markov Decision Process) 입니다.\n",
    "<br>\n",
    "> MDP = { $상태^{\\ state}$, $행동^{\\ action}$, $보상^{\\ reward}$, $정책^{\\ policy}$ }\n",
    "> 5. 순차적 행동 결정 문제를 푸는 과정은 현재 상황에서 에이전트가 더 좋은 정책을 찾는 과정입니다.\n",
    "<br>\n",
    "\n",
    "#### 가치함수 : 에이전트가 어떤 정책이 더 좋은 정책인지 판단하는 기준\n",
    "#### 벨만 방정식 : 현재 상태 가치함수와 다음 상태 가치함수의 관계식\n",
    "##### 벨만 기대 방정식 : $v_{\\pi}(s) = E_{\\pi}[R_{t+1} + \\gamma v_{\\pi}S_{t+1}\\ |\\ S_t = s]$\n",
    "##### 벨만 최적 방정식 : $v_*(s) = max_{a}E[R_{t+1} + \\gamma v_*S_{t+1}\\ |\\ S_t = s, A_t = a]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imitation Learning\n",
    "![RL](images/rl.png)\n",
    "### Supervised learning for decision making\n",
    "> 1. 강화 학습 노테이션을 따르는 입력이 Observation 이고, 출력이 Action 인 지도 학습이라 할 수 있습니다.\n",
    "2. 학습해야 하는 대상은 가운데의 CNN 으로 표현되는 Parameterized Probability Distribution 입니다.\n",
    "3. 어떠한 Observation 들이 등장했을 때에 해야 할 Action 카테고리를 선택하는 문제입니다\n",
    "4. Observations : Partially observed 환경의 에이전트가 관찰할 수 있는 픽셀 값들을 말합니다.\n",
    "5. States : 반면에 Fully observed 이며, 물리적인 위치와 방향 등의 눈으로 관찰되지 않는 정보를 모두 포함합니다.\n",
    "\n",
    "> 하지만 우리의 에이전트들은 모든 정보를 관찰할 수 없기 때문에 states 가 아니라 obs 를 통해 action 을 취하고 다시 이 과정을 반복하게 되며, Markov Property 를 만족해야 하므로 현재 t 의 상태는 다음 t+1 의 상태에만 영향을 줄 수 있으며 t-1 의 상태와는 무관합니다.\n",
    "\n",
    "### 1. Function parameter\n",
    "> 1. 주어진 데이터를 가장 잘 설명하는 \"함수\"를 찾는 알고리즘을 설계하는 과정이라 할 수 있습니다.\n",
    "2. 즉, 입력에 대한 출력 값을 가장 잘 설명해 주는 함수를 추정하는 문제입니다. (function weight or parameter approximation)\n",
    "3. 함수 추정 문제에서는 theta 값이 DNN 의 weights 값이 될 것입니다.\n",
    "\n",
    "### 2. Probability density function paramter\n",
    "> 1. \"함수\" 대신 입력값과 출력값에 대한 확률 분포를 가정하고 이에 따른 확률분포의 parameters 를 유추하는 과정으로 생각할 수 있습니다. \n",
    "2. 입력에 대한 출력 데이터를 가장 잘 설명할 수 있는 mean, covariance 를 찾는 과정입니다. (probability distribution approximation)\n",
    "3. 확률 밀도 추정 문제에서는 theta 값은 주어진 Observation 에 대한 Action 확률 분포의 mean, covariance 값이 될 것입니다.\n",
    "\n",
    "![RL](images/il.png)\n",
    "> 그림에서와 같이 관찰된 데이터에 대해 어떤 행동을 수행하는 지에 대한 조건부 확률분포를 학습하는 과정입니다. 실제 운전 시의 화면과 핸들의 정보를 모두 저장하여 지도학습을 통해 에이전트를 학습 시키는 것이 **\"모방 학습(Imitation Learning)\"**입니다.\n",
    "\n",
    "### Does direct imitation work? ***NO***\n",
    "> 1. 학습 예제에는 없었던 데이터가 출현하는 경우\n",
    "2. 지도학습 데이터에 사람의 실수가 들어간 경우\n",
    "3. 유사한 2개의 이미지에서 다른 행동을 하는 경우\n",
    "4. Convex 문제(Saddle point 등)로 인하여 Big paramter NN 학습에 실패하는 경우\n",
    "![RL](images/no.png)\n",
    "> 어떠한 학습도 완벽할 수는 없고, 아주 작은 오류는 있기 마련입니다. 또한 초반의 작은 오류가 후반으로 갈수록 큰 영향을 주기도 합니다.\n",
    "\n",
    "#### 어떻게 극복할 수 있었는가?\n",
    "> 1. 정면, 측면 카메라 3개 장착과 Bias 통한 핸들링 정보 추가\n",
    "![RL](images/car.png)\n",
    "[End to End Learning for Self-Driving Cars](pages/end-to-end-dl-using-px.pdf)\n",
    "\n",
    "### How can we make it work more often?\n",
    "> 1. 학습 데이터에 의도적으로 노이즈를 섞어서 Stabilizer 효과를 준다\n",
    "![RL](images/stable.png)\n",
    "2. [Iterative LQR(Linear Quadratic Regulaor) Algorithm](https://studywolf.wordpress.com/2016/02/03/the-iterative-linear-quadratic-regulator-method/)\n",
    "3. [RL — LQR & iLQR Linear Quadratic Regulator](https://medium.com/@jonathan_hui/rl-lqr-ilqr-linear-quadratic-regulator-a5de5104c750)\n",
    "\n",
    "#### DAgger: Dataset Aggregation\n",
    "![RL](images/dagger.png)\n",
    "> 1. 학습된 분포를 통해서 데이터를 생성해 냅니다.\n",
    "2. 사람에게 정답을 마킹하게 합니다.\n",
    "3. 이러한 생성된 것과 기존의 학습 데이터를 통합하여 다시 학습 시킵니다\n",
    "\n",
    "#### DAgger의 문제점과 해결방안\n",
    "> 1. 사람이 모든 데이터를 직접 정답을 만들어야 합니다.\n",
    "2. 사람이 항상 옳은 결정을 내리는 것은 아닙니다\n",
    "\n",
    "### Can we make it work without more data?\n",
    "> 데이터를 만들어내지 않고 학습할 수는 없을까?\n",
    "> 1. 초반의 실수가 뒤로 가면 갈 수록 오류가 점점 커지는 문제인 \"분포의 이동의 문제점\"을 가지고 있습니다\n",
    "2. 오류 이동이 없을 만큼 아주 정확하게 학습해야만 합니다\n",
    "3. 사람의 행위를 아주 정확하게 학습하기는 어렵습니다\n",
    "4. 과적합되기 쉽기 때문에 조심해야만 합니다\n",
    "\n",
    "#### Why might we fail to fit the expert?\n",
    "![RL](images/fail.png)\n",
    "> 1. 모든 States 를 안다고 하더라도 사람은 바로 이전의 상태만을 보지 않고 모든 과거 상태를 알고 행동합니다\n",
    "2. 심지어 Markovian Behavior 라고 하더라도 다양한 이유로 인하여 다양한 행동을 할 수 있습니다\n",
    "\n",
    "#### How can we use whole history?\n",
    "![RL](images/cnn.png)\n",
    "![RL](images/rnn.png)\n",
    "[RNN과 LSTM을 이해해보자](https://ratsgo.github.io/natural%20language%20processing/2017/03/09/rnnlstm/)\n",
    "\n",
    "![RL](images/fail0.png)\n",
    "> 1. 기분에 따라 좌.우로 움직이는 경우\n",
    "2. 두 개의 의사결정의 평균이 적절한가?\n",
    "3. Multimodal behavior 는 허용할 수 있으나 Multimodality 는 허용할 수 없다 (둘 다 선택)\n",
    "\n",
    "![RL](images/fail1.png)\n",
    "> 1. [가우시안 혼합 모형과 EM](https://datascienceschool.net/view-notebook/c2934b8a2f9140e4b8364b266b1aa0d8)\n",
    "\n",
    "![RL](images/fail2.png)\n",
    "> 1. [알기쉬운 Variational autoencoder](https://www.slideshare.net/ssuser06e0c5/variational-autoencoder-76552518)\n",
    "\n",
    "![RL](images/fail3.png)\n",
    "> 1. 연속적인 다차원 행위를 불연속적인 행동들로 변환하는 과정을 DNN 통한 학습\n",
    "\n",
    "![RL](images/recap.png)\n",
    "> 1. 단순한 접근으로는 잘 되지 않습니다만, 때때로 잘 동작하기도 합니다\n",
    "1. 추가적인 이미지 및 보정을 하는 경우\n",
    "1. 샘플을 통한 안정적인 trajectory 를 보장하는 경우\n",
    "1. 자동 생성된 많은 데이터를 제공하는 경우\n",
    "1. 보다 정확한 데이터를 위해 수동으로 정답을 제공하는 경우"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Case studies of recent work in (deep) reinforcement learning\n",
    "\n",
    "#### Case study 1. trail following as classification\n",
    "![RL](images/trail1.png)\n",
    "> 1. 드론이 산길을 스스로 가도록 학습합니다\n",
    "1. 야생 탐사나 구조가 필요한 장소에 탐험을 보낼 때에 유용합니다\n",
    "1. 평범한 카메라를 통해 길만을 인지하며 학습합니다\n",
    "\n",
    "![RL](images/trail2.png)\n",
    "![RL](images/trail3.png)\n",
    "![RL](images/trail4.png)\n",
    "![RL](images/trail5.png)\n",
    "![RL](images/trail6.png)\n",
    "![RL](images/trail7.png)\n",
    "<br>\n",
    "\n",
    "#### Case study 2. imitation with LSTM\n",
    "![RL](images/lstm0.png)\n",
    "> 1. 네트워크는 RNN 을 활용\n",
    "1. Non-markovian demonstration 을 학습 대상으로 삼았습니다\n",
    "1. Multi-modality 를 해결하기 위해 Density network for Mixture gaussian model 을 적용합니다\n",
    "1. 이러한 과정을 통해 에이전트를 학습시키고 실제 로봇을 움직이는 것이 과제입니다\n",
    "\n",
    "![RL](images/lstm1.png)\n",
    "![RL](images/lstm2.png)\n",
    "> 1. 테이블 위의 물건을 선반위로 옮기는 것을 학습합니다\n",
    "1. 가능한 정확하게 물건을 집어야 하지만, 완벽한 예제만 데모로 보여주지는 않습니다\n",
    "1. 의도적으로 실수하는 과정을 포함해서 에이전트가 실수 했을 때에 수정하는 과정 또한 학습에 적용하였습니다\n",
    "\n",
    "![RL](images/lstm3.png)\n",
    "![RL](images/lstm4.png)\n",
    "> 1. LSTM 3개의 레이어를 포함하며, Network Output 은 Mixuture of gaussian 입니다\n",
    "1. 비교를 위해 Feed forward 네트워크를 아래에 두었습니다\n",
    " * Feed Forward - Mean Square Error\n",
    " * Long Short Term Memory - MSE\n",
    " * Feed Forward - Mixture Density Network\n",
    " * LSTM - MDN\n",
    "\n",
    "![RL](images/lstm5.png)\n",
    "> 1. 학습을 위해 PS Move 봉을 연동\n",
    "\n",
    "![RL](images/lstm6.png)\n",
    "> 1. 이미지 처리를 위해 CNN 적용\n",
    "1. Non-markovian 처리를 위해 RNN 적용\n",
    "1. Multi-modality 처리를 위한 MDN 적용\n",
    "\n",
    "![RL](images/lstm7.png)\n",
    "> 1. 드라이버 상자를 닦는 동작이나, 상자를 움직이는 행동도 불과 몇 번의 시범을 통해 학습이 가능하다고 합니다\n",
    "\n",
    "### Other topics in imitation learning\n",
    "![RL](images/other.png)\n",
    "> 1. Sturucture prediction\n",
    " * where are you i'm at work 순서가 가장 확률이 높을 것 이며, i'm in work 는 correlation 이 상대적으로 낮을 것입니다\n",
    " * Machine translation 에 사용되는 Langauge model 입니다\n",
    "2. Inverse reinforcment learning\n",
    " * Policy learning setting : 단순히 데모를 복사해서 학습하는 것이 아니라 목적을 이해하는 것이라고 합니다 (?!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. What is missing from imitation learning?\n",
    "\n",
    "#### Imitation Learning : What is the problem?\n",
    "![RL](images/problem.png)\n",
    "> 1. 딥러닝은 데이터가 생명인데, 사람이 제공하는 데이터는 한계가 있고 비싸다 \n",
    "1. 사람은 산길을 내려가기는 잘하지만 드론 조정은 익숙하지 않다\n",
    "1. 사람은 손쉽게 스스로 학습하지만 기계는 가능한가?\n",
    " * 충분한 데이터가 제공되어야 하고 \n",
    " * 지속적인 스스로 학습이 필요하다\n",
    " \n",
    " ### TODO 1:13:00 초부터 다시 듣기 시작"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q&A\n",
    "#### 1. 왜 Function Approximation 이 아니라 Parameterized Probability Distribution 인가?\n",
    "> 어느 쪽을 정한 것이 아니라 네트워크 학습을 어떻게 할 것인지의 문제이므로 Case by case 로 선택 할 수 있는 문제라고 생각합니다\n",
    "\n",
    "#### 2. 왜 RL 에서 Markov Property 가 꼭 적용되어야 하는 것처럼 말하고 있는가?\n",
    "> 그렇지 않으면 계산량이 너무 복잡해서 가정을 한 것이라고 생각합니다\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [py36]",
   "language": "python",
   "name": "Python [py36]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
